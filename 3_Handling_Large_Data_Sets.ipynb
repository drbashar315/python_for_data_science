{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Large Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is 'LARGE'? : Some rough estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 – 1 MB\n",
    "Can visually inspect in Excel.\n",
    "Easy to visualize in Excel. \n",
    "\n",
    "### 1 – 50 MB\n",
    "Can Read and process in Excel on a single computer.\n",
    "50 MB – 1 GB\n",
    "Can easily load and process in MySQL.\n",
    "Excel starts having issues loading into memory.\n",
    "Can process in Python using Pandas.\n",
    "\n",
    "### 1 GB – 100 GB\n",
    "MySQL optimizations / keys start to become a necessity.\n",
    "Excel does not work for most operations.\n",
    "Benefits of on-disk processing become apparent as the dataset exceeds single machine memory.\n",
    "\n",
    "### 100 GB – 10T B \n",
    "Can still be processed on one machine with limitations.\n",
    "Distributed solutions start to become beneficial.\n",
    "\n",
    "### 10 TB - 1 PB\n",
    "Single hard disk drives are not large enough to store entire dataset.\n",
    "Distributed big data solutions start to become necessary.\n",
    "\n",
    "### 1 PB +\n",
    "Enterprise distributed clusters are needed.\n",
    "You will need a full time team to manage data.\n",
    "You will most likely develop custom technologies around your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Bash / Shell Scripts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We can do some very basic operations with the built in tools of Unix /Linux \n",
    "# This would allow us to read directly from disk. Examples:-\n",
    "\n",
    "\n",
    "#1.  Show last 3 lines of wifi_list.csv using:       tail -3 wifi_list.csv\n",
    "#2.  Count the total rows in wifi_list.csv using:    cat wifi_list.csv | wc -l\n",
    "#3.  Show first 3 lines of wifi_list.csv using:      head -3 wifi_list.csv\n",
    "\n",
    "Bash command and output example:\n",
    "-----------\n",
    "$ head -3 wifi_list.csv\n",
    "MAC,SSID,AuthMode,FirstSeen,Channel,RSSI,CurrentLatitude,CurrentLongitude,AltitudeMeters,AccuracyMeters,Type\n",
    "e0:91:f5:de:fa:4f,NETGEAR-5G,[WPS][ESS],5/9/2012 2:02,36,-67,44.67023722,-74.98363701,104.1999969,7,WIFI\n",
    "e2:91:f5:de:fa:51,bonesaw_Guest,[WPA-PSK-TKIP+CCMP][WPA2-PSK-TKIP+CCMP][ESS],5/9/2012 2:02,11,-62,44.67023722,-74.98363701,104.1999969,7,WIFI\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python Read Line By Line Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "# Open wifi.csv iterate through file. \n",
    "# Print the line number WHERE the row contains ‘CPH’\n",
    "with open ('wifi_list.csv', 'r') as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if 'CPH' in line:\n",
    "            print(i)\n",
    "            break\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "# Open wifi.csv iterate through file.\n",
    "# Print the total number of lines which contains ‘CPH’\n",
    "n=0\n",
    "with open ('wifi_list.csv', 'r') as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        if 'CPH' in line:\n",
    "            n+=1\n",
    "            \n",
    "        i+=1\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "# Use the CSV parser to read each line at a time as a list.\n",
    "# Count the number of SSIDs which contain CPH.\n",
    "import csv\n",
    "n=0\n",
    "with open ('wifi_list.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, line in enumerate(reader):\n",
    "        if 'CPH' in line[1]:\n",
    "            n +=1                   \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sending voluminous data to a database\n",
    "\n",
    "### See the previous notebook: 2_Handling_Databases_And_Optimizing_Queries_PyMySQL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block size: 4700 - total time : 12.765304803848267\n"
     ]
    }
   ],
   "source": [
    "# Establishing the connection to a database by PyMySQL\n",
    "\n",
    "\n",
    "import pymysql\n",
    "conn = pymysql.connect(host='http://www.host.name OR localhost', \\\n",
    "                       port=3306, \\\n",
    "                       user='username', \\\n",
    "                       passwd='p@ssword', \\\n",
    "                       db='database_name', autocommit=True)\n",
    "cur = conn.cursor(pymysql.cursors.DictCursor)\n",
    "\n",
    "#cur = conn.cursor() # if DictCursor is not required \n",
    "# about cursor: read more here: (https://eric.lubow.org/2010/when-to-use-mysql-cursor-classes-in-python/)\n",
    "\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO `pvsubset2`\n",
    "(\n",
    "`summons_number`,`plate_id`, `registration_state`,`issue_date`,`violation_code`,\n",
    "`vehicle_body_type`,`vehicle_make`,`vehicle_expiration_date`,`house_number`,`street_name`,`vehicle_color`, \n",
    "`vehicle_year`)\n",
    " VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\n",
    " ''' \n",
    "with open('pvsubset.csv') as f:\n",
    "    data = [{k: str(v) for k, v in row.items()}\n",
    "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
    "tokens =[]\n",
    "n= 0 \n",
    "i=0\n",
    "blocksizes = [4700]\n",
    "\n",
    "for bs in blocksizes:\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    for line in data:\n",
    "        tokens.append((line[\"Summons Number\"],\n",
    "                  line[\"Plate ID\"],\n",
    "                  line[\"Registration State\"],\n",
    "                  line[\"Issue Date\"],\n",
    "                  line[\"Violation Code\"],\n",
    "                  line[\"Vehicle Body Type\"],\n",
    "                  line[\"Vehicle Make\"],\n",
    "                  line[\"Vehicle Expiration Date\"],\n",
    "                  line[\"House Number\"],\n",
    "                  line[\"Street Name\"],\n",
    "                  line[\"Vehicle Color\"],\n",
    "                  line[\"Vehicle Year\"]))\n",
    "        if i % bs == 0:\n",
    "            n+=1\n",
    "            bstart = time.time()\n",
    "            cur.executemany(sql,tokens)\n",
    "            conn.commit()\n",
    "            tokens = []\n",
    "        i+=1\n",
    "    print (\"block size: \" + str(bs) + \" - total time : \" + str(time.time() - start))\n",
    "    if len(tokens) > 0:\n",
    "        cur.executemany(sql,tokens)\n",
    "        conn.commit()\n",
    "    \n",
    "cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
